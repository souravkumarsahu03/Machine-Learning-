{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b1ae622-fbaf-4d76-ad35-76558f84868f",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8bc1fc-a978-4541-b9c2-3c6f67ed6460",
   "metadata": {},
   "source": [
    "### 1) Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97d53064-46b8-414d-a06e-2fca5c0ffb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am Data Scientist and Data Analyst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7abf6df3-d99e-4fe7-ae90-81b8a88343bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am Data Scientist and Data Analyst']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b56683f5-55c2-47bd-9133-2257ccb7f9df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon = set(text.lower().split(' '))\n",
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92c8802-f032-49bf-bb63-55f530fdda2b",
   "metadata": {},
   "source": [
    "### 2) Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93c1353c-b4a5-4746-9043-279a7a766106",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'I am waling in the road'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43eed04e-4639-4061-b583-d247f043c981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autocorrect\n",
      "  Using cached autocorrect-2.6.1.tar.gz (622 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: autocorrect\n",
      "  Building wheel for autocorrect (pyproject.toml): started\n",
      "  Building wheel for autocorrect (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622422 sha256=3a74eae24ec601e75d5c015af11d8a1da876e5b59fcbb42474aa0739804fc542\n",
      "  Stored in directory: c:\\users\\sourav\\appdata\\local\\pip\\cache\\wheels\\b5\\7b\\6d\\b76b29ce11ff8e2521c8c7dd0e5bfee4fb1789d76193124343\n",
      "Successfully built autocorrect\n",
      "Installing collected packages: autocorrect\n",
      "Successfully installed autocorrect-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4deb563-c564-4191-b233-f377f61d014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8c06352-f7b2-4828-b709-0802d21ba7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = Speller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ea197ad1-9e7c-4bd1-860d-0dbe0ad071ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am walking in the road'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell.autocorrect_sentence(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9623fc-f495-4730-b5cd-e48830592e87",
   "metadata": {},
   "source": [
    "### 3) Emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cdc2f565-0ee7-4333-b546-97148896fa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Using cached emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Using cached emoji-2.15.0-py3-none-any.whl (608 kB)\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15fe4e3e-b270-4a41-b39a-a6d4ddf17086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73479bf6-b2f4-42dc-b768-dff68e2bd859",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'ðŸ˜’'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d1673216-675d-4f5e-990e-95daa44a2706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unamused face'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji.demojize(data).replace(':','').replace('_',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a790fa-9074-483d-b1f2-8423258773ea",
   "metadata": {},
   "source": [
    "### 4) Chat words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "937e795f-b44e-4011-be2d-a134cc9c8b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\SOURAV\\Downloads\\4743416-text_filess\\chat words.txt\", mode='r') as file:\n",
    "    chat_words = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad90abe2-91b3-45df-91a2-dfd6b454377a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"brb\": \"be right back\",\n",
      "    \"gtg\": \"got to go\",\n",
      "    \"ttyl\": \"talk to you later\",\n",
      "    \"imo\": \"in my opinion\",\n",
      "    \"imho\": \"in my humble opinion\",\n",
      "    \"fyi\": \"for your information\",\n",
      "    \"smh\": \"shaking my head\",\n",
      "    \"tbh\": \"to be honest\",\n",
      "    \"idk\": \"i don't know\",\n",
      "    \"lol\": \"laugh out loud\",\n",
      "    \"rofl\": \"rolling on the floor laughing\",\n",
      "    \"lmao\": \"laughing my ass off\",\n",
      "    \"btw\": \"by the way\",\n",
      "    \"omg\": \"oh my god\",\n",
      "    \"afk\": \"away from keyboard\",\n",
      "    \"nvm\": \"never mind\",\n",
      "    \"ily\": \"i love you\",\n",
      "    \"wyd\": \"what you doing?\",\n",
      "    \"wbu\": \"what about you?\",\n",
      "    \"rn\": \"right now\",\n",
      "    \"jk\": \"just kidding\",\n",
      "    \"hbu\": \"how about you?\",\n",
      "    \"thx\": \"thanks\",\n",
      "    \"ty\": \"thank you\",\n",
      "    \"yw\": \"you're welcome\",\n",
      "    \"np\": \"no problem\",\n",
      "    \"u\": \"you\",\n",
      "    \"ur\": \"your\",\n",
      "    \"r\": \"are\",\n",
      "    \"y\": \"why\",\n",
      "    \"plz\": \"please\",\n",
      "    \"pls\": \"please\",\n",
      "    \"bc\": \"because\",\n",
      "    \"cuz\": \"because\",\n",
      "    \"coz\": \"because\",\n",
      "    \"afaik\": \"as far as I know\",\n",
      "    \"asap\": \"as soon as possible\",\n",
      "    \"atm\": \"at the moment\",\n",
      "    \"bbl\": \"be back later\",\n",
      "    \"bfn\": \"bye for now\",\n",
      "    \"bff\": \"best friends forever\",\n",
      "    \"cu\": \"see you\",\n",
      "    \"cya\": \"see you\",\n",
      "    \"dm\": \"direct message\",\n",
      "    \"fb\": \"Facebook\",\n",
      "    \"ftw\": \"for the win\",\n",
      "    \"gg\": \"good game\",\n",
      "    \"gr8\": \"great\",\n",
      "    \"hax\": \"hacks\",\n",
      "    \"ikr\": \"i know, right?\",\n",
      "    \"lmk\": \"let me know\",\n",
      "    \"omw\": \"on my way\",\n",
      "    \"ppl\": \"people\",\n",
      "    \"wp\": \"well played\",\n",
      "    \"op\": \"overpowered\",\n",
      "    \"nerf\": \"weaken\",\n",
      "    \"noob\": \"newbie\",\n",
      "    \"ez\": \"easy\",\n",
      "    \"fomo\": \"fear of missing out\",\n",
      "    \"tbt\": \"throwback thursday\",\n",
      "    \"fam\": \"close friends\",\n",
      "    \"sus\": \"suspicious\",\n",
      "    \"clout\": \"social influence\",\n",
      "    \"bae\": \"babe\",\n",
      "    \"luv\": \"love\",\n",
      "    \"xoxo\": \"hugs and kisses\",\n",
      "    \"muah\": \"kissing sound\",\n",
      "    \"smooch\": \"kiss\",\n",
      "    \"crushin\": \"having a crush\",\n",
      "    \"wfh\": \"work from home\",\n",
      "    \"eod\": \"end of day\",\n",
      "    \"tl;dr\": \"too long; didnÃ¢â‚¬â„¢t read\",\n",
      "    \"4\": \"for\",\n",
      "    \"b4\": \"before\",\n",
      "    \"w8\": \"wait\",\n",
      "    \"2day\": \"today\",\n",
      "    \"2moro\": \"tomorrow\",\n",
      "    \"tmr\": \"tomorrow\",\n",
      "    \"dunno\": \"don't know\",\n",
      "    \"cud\": \"could\",\n",
      "    \"shud\": \"should\",\n",
      "    \"lemme\": \"let me\",\n",
      "    \"gimme\": \"give me\",\n",
      "    \"imma\": \"i am going to\",\n",
      "    \"prolly\": \"probably\",\n",
      "    \":)\": \"smile\",\n",
      "    \":(\": \"sad\",\n",
      "    \":D\": \"very happy\",\n",
      "    \"XD\": \"laughing face\",\n",
      "    \":P\": \"playful\",\n",
      "    \":'(\": \"crying\",\n",
      "    \":O\": \"shocked\",\n",
      "    \";)\": \"wink\",\n",
      "    \"^_^\": \"happy\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(chat_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0e774e7e-8cd0-43ef-813a-596fbc2a0bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words = {\n",
    "    \"brb\": \"be right back\",\n",
    "    \"gtg\": \"got to go\",\n",
    "    \"ttyl\": \"talk to you later\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"imho\": \"in my humble opinion\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"idk\": \"i don't know\",\n",
    "    \"lol\": \"laugh out loud\",\n",
    "    \"rofl\": \"rolling on the floor laughing\",\n",
    "    \"lmao\": \"laughing my ass off\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"afk\": \"away from keyboard\",\n",
    "    \"nvm\": \"never mind\",\n",
    "    \"ily\": \"i love you\",\n",
    "    \"wyd\": \"what you doing?\",\n",
    "    \"wbu\": \"what about you?\",\n",
    "    \"rn\": \"right now\",\n",
    "    \"jk\": \"just kidding\",\n",
    "    \"hbu\": \"how about you?\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"ty\": \"thank you\",\n",
    "    \"yw\": \"you're welcome\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"u\": \"you\",\n",
    "    \"ur\": \"your\",\n",
    "    \"r\": \"are\",\n",
    "    \"y\": \"why\",\n",
    "    \"plz\": \"please\",\n",
    "    \"pls\": \"please\",\n",
    "    \"bc\": \"because\",\n",
    "    \"cuz\": \"because\",\n",
    "    \"coz\": \"because\",\n",
    "    \"afaik\": \"as far as I know\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"atm\": \"at the moment\",\n",
    "    \"bbl\": \"be back later\",\n",
    "    \"bfn\": \"bye for now\",\n",
    "    \"bff\": \"best friends forever\",\n",
    "    \"cu\": \"see you\",\n",
    "    \"cya\": \"see you\",\n",
    "    \"dm\": \"direct message\",\n",
    "    \"fb\": \"Facebook\",\n",
    "    \"ftw\": \"for the win\",\n",
    "    \"gg\": \"good game\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"hax\": \"hacks\",\n",
    "    \"ikr\": \"i know, right?\",\n",
    "    \"lmk\": \"let me know\",\n",
    "    \"omw\": \"on my way\",\n",
    "    \"ppl\": \"people\",\n",
    "    \"wp\": \"well played\",\n",
    "    \"op\": \"overpowered\",\n",
    "    \"nerf\": \"weaken\",\n",
    "    \"noob\": \"newbie\",\n",
    "    \"ez\": \"easy\",\n",
    "    \"fomo\": \"fear of missing out\",\n",
    "    \"tbt\": \"throwback thursday\",\n",
    "    \"fam\": \"close friends\",\n",
    "    \"sus\": \"suspicious\",\n",
    "    \"clout\": \"social influence\",\n",
    "    \"bae\": \"babe\",\n",
    "    \"luv\": \"love\",\n",
    "    \"xoxo\": \"hugs and kisses\",\n",
    "    \"muah\": \"kissing sound\",\n",
    "    \"smooch\": \"kiss\",\n",
    "    \"crushin\": \"having a crush\",\n",
    "    \"wfh\": \"work from home\",\n",
    "    \"eod\": \"end of day\",\n",
    "    \"tl;dr\": \"too long; didnÃ¢â‚¬â„¢t read\",\n",
    "    \"4\": \"for\",\n",
    "    \"b4\": \"before\",\n",
    "    \"w8\": \"wait\",\n",
    "    \"2day\": \"today\",\n",
    "    \"2moro\": \"tomorrow\",\n",
    "    \"tmr\": \"tomorrow\",\n",
    "    \"dunno\": \"don't know\",\n",
    "    \"cud\": \"could\",\n",
    "    \"shud\": \"should\",\n",
    "    \"lemme\": \"let me\",\n",
    "    \"gimme\": \"give me\",\n",
    "    \"imma\": \"i am going to\",\n",
    "    \"prolly\": \"probably\",\n",
    "    \":)\": \"smile\",\n",
    "    \":(\": \"sad\",\n",
    "    \":D\": \"very happy\",\n",
    "    \"XD\": \"laughing face\",\n",
    "    \":P\": \"playful\",\n",
    "    \":'(\": \"crying\",\n",
    "    \":O\": \"shocked\",\n",
    "    \";)\": \"wink\",\n",
    "    \"^_^\": \"happy\",\n",
    "    \"tq\": \"thank you\",\n",
    "    \"tc\": \"take care\",\n",
    "    \"nd\": \"and\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "272ce5c5-23d9-442d-b6b2-ed1cc9d42d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'omg i don not know you but can you give me this tq nd tc.'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'omg i don not know you but can you give me this tq nd tc.'\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0256be40-1103-42e4-a3b4-78a2020df8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh my god i don not know you but can you give me this thank you and tc.\n"
     ]
    }
   ],
   "source": [
    "data = text.split(' ')\n",
    "for i in data:\n",
    "    if i in chat_words.keys():\n",
    "        text = text.replace(i, chat_words[i])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b1a0bef2-1ca8-41aa-a909-f0eaabb19479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh my god i don not know you but can you give me this thank you and tc.\n"
     ]
    }
   ],
   "source": [
    "for word in text.split(' '):\n",
    "    if word in chat_words.keys():\n",
    "        text = text.replace(word, chat_words[word])\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ee49b9-47c2-4d93-8ec5-10a2b13fee0e",
   "metadata": {},
   "source": [
    "### 5) Contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7afafeaa-68e0-4962-9a0d-d018713819d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I've been waitinf here but i don't know whether he'll come or not\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I've been waitinf here but i don't know whether he'll come or not\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "49322640-48ac-4ada-b2ab-a9920c3c530b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Using cached contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting textsearch>=0.0.21 (from contractions)\n",
      "  Using cached textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
      "  Using cached anyascii-0.3.3-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
      "  Downloading pyahocorasick-2.3.0-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Using cached contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Using cached textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached anyascii-0.3.3-py3-none-any.whl (345 kB)\n",
      "Downloading pyahocorasick-2.3.0-cp310-cp310-win_amd64.whl (35 kB)\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "\n",
      "   ---------- ----------------------------- 1/4 [anyascii]\n",
      "   ---------- ----------------------------- 1/4 [anyascii]\n",
      "   ------------------------------ --------- 3/4 [contractions]\n",
      "   ---------------------------------------- 4/4 [contractions]\n",
      "\n",
      "Successfully installed anyascii-0.3.3 contractions-0.1.73 pyahocorasick-2.3.0 textsearch-0.0.24\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e06ae4d2-5ca3-48a9-b1c2-a920fd972800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "04863b54-f46f-430c-996e-9f33efabb343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have been waitinf here but i do not know whether he will come or not'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contractions.fix(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4068f8bd-1fbe-48b3-88df-4a59000f89b4",
   "metadata": {},
   "source": [
    "### 6) Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "de2fb8f7-7659-4a4e-b24a-4028c8108056",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I am @Sourav Kumar Sahu...^^'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "32108c6e-5544-49e9-9d16-0ccf91076822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am Sourav Kumar Sahu\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "for i in text:\n",
    "    if i in string.punctuation:\n",
    "        text = text.replace(i,'')\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acbc247-2cec-405e-b870-36a6882a2ca3",
   "metadata": {},
   "source": [
    "### 7) Removing Accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a257d599-1b73-4351-92ed-c4782b0caa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a281758d-ab32-4a3e-840b-8f8d4bff88a0",
   "metadata": {},
   "source": [
    "### 8) Handling URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5dc20ed4-d9d3-49f3-bebd-b144cd82958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\SOURAV\\Downloads\\4743416-text_filess\\URL Data.txt\", mode='r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2d66120f-b777-42d4-83f9-5e2b8b8439de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'urls = \"\"\"\\nCheck out my blog at https://www.myblog.com for more posts. You can also visit my profile on https://www.linkedin.com/in/johndoe for professional updates. \\nI recently discovered a great article on https://medium.com/@johnny/technology-and-innovation-abc1234. You can find more about my projects on https://github.com/johndoe. \\nAlso, follow us on https://www.twitter.com/techhub for the latest news in tech. Don\\'t forget to subscribe to my YouTube channel at https://www.youtube.com/channel/UC12345678. \\nFor any inquiries, please contact me at https://www.contactme.com.\\n\"\"\"\\n'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4cb0451d-2074-4f3d-9f37-b2c283bc1808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urls = \"\"\"\n",
      "Check out my blog at  for more posts. You can also visit my profile on  for professional updates. \n",
      "I recently discovered a great article on  You can find more about my projects on  \n",
      "Also, follow us on  for the latest news in tech. Don't forget to subscribe to my YouTube channel at  \n",
      "For any inquiries, please contact me at \n",
      "\"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = r'(www.|http:|https:)\\S+'\n",
    "replace = ''\n",
    "print(re.sub(pattern, replace, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eeea64-7e34-4828-9e26-e5ebe7a586e3",
   "metadata": {},
   "source": [
    "# Advanced Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1741e945-b10e-459a-a63f-692f2575be34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: click in c:\\anaconda\\envs\\cv_fix_env\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\anaconda\\envs\\cv_fix_env\\lib\\site-packages (from nltk) (1.5.3)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2026.1.15-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda\\envs\\cv_fix_env\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\envs\\cv_fix_env\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "Downloading regex-2026.1.15-cp310-cp310-win_amd64.whl (277 kB)\n",
      "Installing collected packages: regex, nltk\n",
      "\n",
      "   ---------------------------------------- 0/2 [regex]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   ---------------------------------------- 2/2 [nltk]\n",
      "\n",
      "Successfully installed nltk-3.9.2 regex-2026.1.15\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2bb3e97a-82b2-4cce-9f12-ff3b5702db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a13b87ea-dff1-4621-a4c2-ea25143365a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'i am data scientist ,and data analyst'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "8abc4293-3440-42e6-97b5-fb2a0f09279a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'data', 'scientist', ',', 'and', 'data', 'analyst']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8801c391-9015-4212-9d89-b686cb8ec053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i am data scientist ,and data analyst']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a8de56-b045-4322-b7d8-077080c2c86a",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b05d9930-cf1d-4c77-9788-2199a7264139",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'i am data scientist and data analyst'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9107591b-2ac7-41d9-ab45-61b34cd4a8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f0da19b9-d0a7-4573-b83a-657bce00fd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'scientist', 'data', 'analyst']\n"
     ]
    }
   ],
   "source": [
    "token_words = word_tokenize(text)\n",
    "output = []\n",
    "for words in token_words:\n",
    "    # print(words)\n",
    "    if words not in stopwords.words('english'):\n",
    "        output.append(words)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b250e0e-8b89-4c91-b7c9-92b085b5a26d",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0d9aac7d-d51b-4851-a98f-2ae2b0fe12ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c1ea657d-e9ab-461b-ba75-50f1c4812352",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "07b52345-c42b-4643-afd8-22954b67f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'beautifully'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "841f5752-9476-4c7e-9ba4-be7b2f10066d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beauti'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "51279aa5-1477-4e84-878c-de9cfce62a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d40af417-8e09-4cf8-8e1c-7320f74dc263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beauty'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls.stem(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "908d9e11-26ec-4682-8d9f-e2f00cdb2603",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "87e0916f-93fe-455b-81c6-bb8ebb63edaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beauti'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sb.stem(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "05b2db74-ade8-4e75-bc96-2a2756706804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9953d143-d997-4421-99ae-1f11d9611a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\SOURAV\\Downloads\\4743416-text_filess\\sample_text_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "750b45bb-59ad-44fc-b2f2-2aebb101e2bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordNetLemmatizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[146], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_preprocess\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\cv_fix_env\\lib\\site-packages\\pandas\\core\\series.py:4943\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4808\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4809\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4810\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4815\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4816\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4817\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4818\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4819\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4934\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4936\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4937\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4939\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4941\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\cv_fix_env\\lib\\site-packages\\pandas\\core\\apply.py:1422\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\cv_fix_env\\lib\\site-packages\\pandas\\core\\apply.py:1502\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1500\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1501\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1502\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1507\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1508\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\cv_fix_env\\lib\\site-packages\\pandas\\core\\base.py:925\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\cv_fix_env\\lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mpandas/_libs/lib.pyx:2999\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[145], line 4\u001b[0m, in \u001b[0;36mtext_preprocess\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      2\u001b[0m speller \u001b[38;5;241m=\u001b[39m autocorrect\u001b[38;5;241m.\u001b[39mSpeller()\n\u001b[0;32m      3\u001b[0m stem \u001b[38;5;241m=\u001b[39m PorterStemmer()\n\u001b[1;32m----> 4\u001b[0m lemma \u001b[38;5;241m=\u001b[39m \u001b[43mWordNetLemmatizer\u001b[49m()\n\u001b[0;32m      5\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()   \u001b[38;5;66;03m#converting to Lower Case\u001b[39;00m\n\u001b[0;32m      6\u001b[0m text \u001b[38;5;241m=\u001b[39m speller\u001b[38;5;241m.\u001b[39mautocorrect_sentence(text)   \u001b[38;5;66;03m#Corecting the Spelling Mistakes\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'WordNetLemmatizer' is not defined"
     ]
    }
   ],
   "source": [
    "df['text'].apply(text_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e0511a86-58c3-490d-83ec-8c8e2bdd2916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    speller = autocorrect.Speller()\n",
    "    stem = PorterStemmer()\n",
    "    lemma = WordNetLemmatizer()\n",
    "    text = text.lower()   #converting to Lower Case\n",
    "    text = speller.autocorrect_sentence(text)   #Corecting the Spelling Mistakes\n",
    "    text = contractions.fix(text)   # dealing with contraction\n",
    "    text = emoji.demojize(text).replace(':',' ')   #Emoji Prediction\n",
    "    text = re.sub(r'<.*?>',' ',text)  #HTML Tags\n",
    "    text = re.sub(r'(www.|http|https)\\+S',' ',text)  #Replacing the URLS\n",
    "    text = re.sub(r\"[^a-zA-Z0-9']\",' ',text)  #Removing the punctuation marks\n",
    "    text = re.sub(r'[0-9] ','',text)    #replacing the Numbers\n",
    "    \n",
    "    text = ' '.join(map(lambda i: chat_words[i] if i in chat_words.keys() else i, text.split()))  #Chat Word Prediction\n",
    "    \n",
    "    text = word_tokenize(text)   #Word Tokenization\n",
    "    text = [i for i in text if i not in stopwords.words('english') ]  #Stop Words Removal\n",
    "    text = [stem.stem(i) for i in text]   #Stemming\n",
    "    text = [lemma.lemmatize(i)for i in text]  #Lemmatization\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "41ea36d1-df3c-47af-b11c-0c44a43b0ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748415bf-0161-479c-bc16-0ba5a0c711d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
