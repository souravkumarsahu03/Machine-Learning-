{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bb9d3fa-9061-4f02-b109-2dd294026355",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1fa6b1-a9dc-428e-9e55-69b4cd7262b2",
   "metadata": {},
   "source": [
    "### 1. Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2de81333-5e9c-4dd0-a61c-29a70d30bec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'I am Data scientist and data Analyst'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8963648-12c3-478c-b1ff-c44f63abf163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(text.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "675f1fa2-2059-4ba9-9fb9-5d73bcb657cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(text.lower().split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43802fb1-6a0a-4bf8-9832-e09d5f657cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'am', 'analyst', 'and', 'data', 'i', 'scientist'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon = set(text.lower().split(' '))\n",
    "lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2efe948-d37e-4ff8-a5f0-2340852b2437",
   "metadata": {},
   "source": [
    "### 2. Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "319914bc-aadf-4141-ac16-05c455d3c200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autocorrect\n",
      "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
      "     ---------------------------------------- 0.0/622.8 kB ? eta -:--:--\n",
      "     -------------------------------------- 622.8/622.8 kB 8.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: autocorrect\n",
      "  Building wheel for autocorrect (setup.py): started\n",
      "  Building wheel for autocorrect (setup.py): finished with status 'done'\n",
      "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622422 sha256=e558c423fb2de29354132bf78ffd297a388366964b67f0bf602c3e6f5a380551\n",
      "  Stored in directory: c:\\users\\sourav\\appdata\\local\\pip\\cache\\wheels\\2c\\2b\\89\\33dd563fa4fa5216ea4b095313a33b7ebc7de8b6cedddfe9f3\n",
      "Successfully built autocorrect\n",
      "Installing collected packages: autocorrect\n",
      "Successfully installed autocorrect-2.6.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'autocorrect' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'autocorrect'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41a14e57-d1cd-437c-a4fd-ca785f8836ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autocorrect'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mautocorrect\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Speller\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'autocorrect'"
     ]
    }
   ],
   "source": [
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "634578c6-4c63-478d-9360-cc3ce4540764",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'maningful'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6de65b8b-447a-49f7-8974-2a345ff853bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = Speller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea12b8a5-95a3-4fc7-91ec-fb8d04e7ba45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meaningful'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell.autocorrect_word(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "82f5bb18-d7f1-41b0-8fef-fe0e3d7577e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'He is walkng on the raod'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8e7d11d6-b477-4e6b-89c9-410f4c509b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He is walking on the road'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell.autocorrect_sentence(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c9f4be-fdc0-4de7-b047-1f09b3d0483b",
   "metadata": {},
   "source": [
    "### 3. Emoji Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "56e71e72-3e51-4ece-9321-89de62a760a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\anaconda\\envs\\data_science\\lib\\site-packages (2.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7b4ae1be-2583-4cf9-971d-6419a9442a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8c7a0975-7d59-4866-8cda-7c3200911510",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'üëô'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "db78571d-726e-4d7d-843e-ebe81f952e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = emoji.demojize(text, delimiters=('','')).replace('_',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "06c08987-ed0a-4476-8054-22f3245cc056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bikini'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fb1240-a6d7-424b-9a81-44214f8f3b97",
   "metadata": {},
   "source": [
    "### 4. Chat Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ce0e9288-292c-4834-93a7-d1da47c96201",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'omg i do not know you give this me tq and tc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a5a649a0-b05b-495f-9e87-78b985d2fe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\SOURAV\\Downloads\\4743416-text_filess\\chat words.txt\", mode='r') as file:\n",
    "    chat_words = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e58de4fb-1063-4296-9d3f-d79a639ffecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words = {\n",
    "    \"brb\": \"be right back\",\n",
    "    \"gtg\": \"got to go\",\n",
    "    \"ttyl\": \"talk to you later\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"imho\": \"in my humble opinion\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"idk\": \"i don't know\",\n",
    "    \"lol\": \"laugh out loud\",\n",
    "    \"rofl\": \"rolling on the floor laughing\",\n",
    "    \"lmao\": \"laughing my ass off\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"afk\": \"away from keyboard\",\n",
    "    \"nvm\": \"never mind\",\n",
    "    \"ily\": \"i love you\",\n",
    "    \"wyd\": \"what you doing?\",\n",
    "    \"wbu\": \"what about you?\",\n",
    "    \"rn\": \"right now\",\n",
    "    \"jk\": \"just kidding\",\n",
    "    \"hbu\": \"how about you?\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"ty\": \"thank you\",\n",
    "    \"yw\": \"you're welcome\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"u\": \"you\",\n",
    "    \"ur\": \"your\",\n",
    "    \"r\": \"are\",\n",
    "    \"y\": \"why\",\n",
    "    \"plz\": \"please\",\n",
    "    \"pls\": \"please\",\n",
    "    \"bc\": \"because\",\n",
    "    \"cuz\": \"because\",\n",
    "    \"coz\": \"because\",\n",
    "    \"afaik\": \"as far as I know\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"atm\": \"at the moment\",\n",
    "    \"bbl\": \"be back later\",\n",
    "    \"bfn\": \"bye for now\",\n",
    "    \"bff\": \"best friends forever\",\n",
    "    \"cu\": \"see you\",\n",
    "    \"cya\": \"see you\",\n",
    "    \"dm\": \"direct message\",\n",
    "    \"fb\": \"Facebook\",\n",
    "    \"ftw\": \"for the win\",\n",
    "    \"gg\": \"good game\",\n",
    "    \"gr8\": \"great\",\n",
    "    \"hax\": \"hacks\",\n",
    "    \"ikr\": \"i know, right?\",\n",
    "    \"lmk\": \"let me know\",\n",
    "    \"omw\": \"on my way\",\n",
    "    \"ppl\": \"people\",\n",
    "    \"wp\": \"well played\",\n",
    "    \"op\": \"overpowered\",\n",
    "    \"nerf\": \"weaken\",\n",
    "    \"noob\": \"newbie\",\n",
    "    \"ez\": \"easy\",\n",
    "    \"fomo\": \"fear of missing out\",\n",
    "    \"tbt\": \"throwback thursday\",\n",
    "    \"fam\": \"close friends\",\n",
    "    \"sus\": \"suspicious\",\n",
    "    \"clout\": \"social influence\",\n",
    "    \"bae\": \"babe\",\n",
    "    \"luv\": \"love\",\n",
    "    \"xoxo\": \"hugs and kisses\",\n",
    "    \"muah\": \"kissing sound\",\n",
    "    \"smooch\": \"kiss\",\n",
    "    \"crushin\": \"having a crush\",\n",
    "    \"wfh\": \"work from home\",\n",
    "    \"eod\": \"end of day\",\n",
    "    \"tl;dr\": \"too long; didn‚Äôt read\",\n",
    "    \"4\": \"for\",\n",
    "    \"b4\": \"before\",\n",
    "    \"w8\": \"wait\",\n",
    "    \"2day\": \"today\",\n",
    "    \"2moro\": \"tomorrow\",\n",
    "    \"tmr\": \"tomorrow\",\n",
    "    \"dunno\": \"don't know\",\n",
    "    \"cud\": \"could\",\n",
    "    \"shud\": \"should\",\n",
    "    \"lemme\": \"let me\",\n",
    "    \"gimme\": \"give me\",\n",
    "    \"imma\": \"i am going to\",\n",
    "    \"prolly\": \"probably\",\n",
    "    \":)\": \"smile\",\n",
    "    \":(\": \"sad\",\n",
    "    \":D\": \"very happy\",\n",
    "    \"XD\": \"laughing face\",\n",
    "    \":P\": \"playful\",\n",
    "    \":'(\": \"crying\",\n",
    "    \":O\": \"shocked\",\n",
    "    \";)\": \"wink\",\n",
    "    \"^_^\": \"happy\",\n",
    "    \"tq\": \"thank you\",\n",
    "    \"tc\": \"take care\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e8178668-1e28-4e2f-8e67-cc6bce8acb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh my god i do not know you give this me thank you and take care\n"
     ]
    }
   ],
   "source": [
    "for word in text.split(' '):\n",
    "    if word in chat_words.keys():\n",
    "        text = text.replace(word, chat_words[word])\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ead56e4-1f62-4ad7-8902-56360cbde873",
   "metadata": {},
   "source": [
    "### 5. Contraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "770fa056-a60f-41cc-932d-f2eb2741570f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting textsearch>=0.0.21 (from contractions)\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
      "  Downloading anyascii-0.3.3-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
      "  Downloading pyahocorasick-2.2.0-cp313-cp313-win_amd64.whl.metadata (14 kB)\n",
      "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading anyascii-0.3.3-py3-none-any.whl (345 kB)\n",
      "Downloading pyahocorasick-2.2.0-cp313-cp313-win_amd64.whl (35 kB)\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "\n",
      "   ---------- ----------------------------- 1/4 [anyascii]\n",
      "   ---------- ----------------------------- 1/4 [anyascii]\n",
      "   ------------------------------ --------- 3/4 [contractions]\n",
      "   ---------------------------------------- 4/4 [contractions]\n",
      "\n",
      "Successfully installed anyascii-0.3.3 contractions-0.1.73 pyahocorasick-2.2.0 textsearch-0.0.24\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "16d3a927-69fc-4f6c-802d-0181e7795838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1dcb190b-a42f-4a3c-978d-b0228a79c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I've been waiting here from longback. But i don't know whether he comes or not\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "32b7b6a8-4e71-4dd1-9855-5815b5806b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I have been waiting here from longback. But i do not know whether he comes or not'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contractions.fix(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e59173-7b5b-4d54-9440-fa8c7a8e5d61",
   "metadata": {},
   "source": [
    "### 6. Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9e9573a4-91a3-4520-ad8a-9826c27db3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b5914bf2-5ce2-4f60-a056-e0c695d7d57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c143eeb9-bb20-4c16-ae11-446f17ace1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am learning NLP @Innomatics ..from $innomatics *ment!ors \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "27176b56-e919-4030-9625-cf94d4609e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am learning NLP Innomatics from innomatics mentors \n"
     ]
    }
   ],
   "source": [
    "for i in text:\n",
    "    if i in string.punctuation:\n",
    "        text = text.replace(i,'')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "979d3aea-0ebd-47e4-9652-db588a199683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "62e034cc-2823-4951-9267-bd6d31055cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'[^a-zA-Z0-9]'\n",
    "replace = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4a0f2577-5839-4718-9ff6-50803337705d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am learning NLP Innomatics from innomatics mentors '"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(pattern, replace, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cb5c7c-f510-45d3-8d38-3209b513710b",
   "metadata": {},
   "source": [
    "### 7. Removing Accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ea7e467-8fab-44d5-bdc4-f8ef9f16fce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textacy\n",
      "  Using cached textacy-0.13.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting cachetools>=4.0.0 (from textacy)\n",
      "  Using cached cachetools-6.2.3-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting catalogue~=2.0 (from textacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting cytoolz>=0.10.1 (from textacy)\n",
      "  Using cached cytoolz-1.1.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting floret~=0.10.0 (from textacy)\n",
      "  Using cached floret-0.10.5-cp312-cp312-win_amd64.whl.metadata (3.2 kB)\n",
      "Collecting jellyfish>=0.8.0 (from textacy)\n",
      "  Using cached jellyfish-1.2.1-cp312-cp312-win_amd64.whl.metadata (642 bytes)\n",
      "Requirement already satisfied: joblib>=0.13.0 in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from textacy) (1.4.2)\n",
      "Collecting networkx>=2.7 (from textacy)\n",
      "  Using cached networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\anaconda\\envs\\computervision\\lib\\site-packages (from textacy) (1.26.4)\n",
      "Collecting pyphen>=0.10.0 (from textacy)\n",
      "  Using cached pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from textacy) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from textacy) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from textacy) (1.5.2)\n",
      "Collecting spacy~=3.0 (from textacy)\n",
      "  Using cached spacy-3.8.11-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting tqdm>=4.19.6 (from textacy)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting toolz>=0.8.0 (from cytoolz>=0.10.1->textacy)\n",
      "  Using cached toolz-1.1.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.10.0->textacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.10.0->textacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.10.0->textacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from requests>=2.10.0->textacy) (2024.12.14)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn>=1.0->textacy) (3.5.0)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy~=3.0->textacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy~=3.0->textacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy~=3.0->textacy)\n",
      "  Using cached murmurhash-1.0.15-cp312-cp312-win_amd64.whl.metadata (2.3 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy~=3.0->textacy)\n",
      "  Using cached cymem-2.0.13-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy~=3.0->textacy)\n",
      "  Using cached preshed-3.0.12-cp312-cp312-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy~=3.0->textacy)\n",
      "  Using cached thinc-8.3.10-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy~=3.0->textacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy~=3.0->textacy)\n",
      "  Using cached srsly-2.5.2-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting weasel<0.5.0,>=0.4.2 (from spacy~=3.0->textacy)\n",
      "  Using cached weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer-slim<1.0.0,>=0.3.0 (from spacy~=3.0->textacy)\n",
      "  Using cached typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy~=3.0->textacy)\n",
      "  Using cached pydantic-2.12.5-py3-none-any.whl.metadata (90 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from spacy~=3.0->textacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from spacy~=3.0->textacy) (75.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from spacy~=3.0->textacy) (24.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.19.6->textacy) (0.4.6)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy)\n",
      "  Using cached pydantic_core-2.41.5-cp312-cp312-win_amd64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\anaconda\\envs\\computervision\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (4.15.0)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy)\n",
      "  Using cached typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy~=3.0->textacy)\n",
      "  Using cached blis-1.3.3-cp312-cp312-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy~=3.0->textacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy~=3.0->textacy) (8.1.7)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy~=3.0->textacy)\n",
      "  Using cached cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.4.2->spacy~=3.0->textacy)\n",
      "  Using cached smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from jinja2->spacy~=3.0->textacy) (3.0.2)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy~=3.0->textacy)\n",
      "  Using cached wrapt-2.0.1-cp312-cp312-win_amd64.whl.metadata (9.2 kB)\n",
      "Using cached textacy-0.13.0-py3-none-any.whl (210 kB)\n",
      "Using cached cachetools-6.2.3-py3-none-any.whl (11 kB)\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached cytoolz-1.1.0-cp312-cp312-win_amd64.whl (947 kB)\n",
      "Using cached floret-0.10.5-cp312-cp312-win_amd64.whl (243 kB)\n",
      "Using cached jellyfish-1.2.1-cp312-cp312-win_amd64.whl (213 kB)\n",
      "Using cached networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "Using cached pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
      "Using cached spacy-3.8.11-cp312-cp312-win_amd64.whl (14.2 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached cymem-2.0.13-cp312-cp312-win_amd64.whl (40 kB)\n",
      "Using cached murmurhash-1.0.15-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Using cached preshed-3.0.12-cp312-cp312-win_amd64.whl (118 kB)\n",
      "Using cached pydantic-2.12.5-py3-none-any.whl (463 kB)\n",
      "Using cached pydantic_core-2.41.5-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Using cached srsly-2.5.2-cp312-cp312-win_amd64.whl (654 kB)\n",
      "Using cached thinc-8.3.10-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "Using cached toolz-1.1.0-py3-none-any.whl (58 kB)\n",
      "Using cached typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.3-py3-none-any.whl (50 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached blis-1.3.3-cp312-cp312-win_amd64.whl (6.2 MB)\n",
      "Using cached cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Using cached typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached wrapt-2.0.1-cp312-cp312-win_amd64.whl (60 kB)\n",
      "Installing collected packages: wrapt, wasabi, typing-inspection, tqdm, toolz, spacy-loggers, spacy-legacy, pyphen, pydantic-core, networkx, murmurhash, jellyfish, floret, cymem, cloudpathlib, catalogue, cachetools, blis, annotated-types, typer-slim, srsly, smart-open, pydantic, preshed, cytoolz, confection, weasel, thinc, spacy, textacy\n",
      "Successfully installed annotated-types-0.7.0 blis-1.3.3 cachetools-6.2.3 catalogue-2.0.10 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.13 cytoolz-1.1.0 floret-0.10.5 jellyfish-1.2.1 murmurhash-1.0.15 networkx-3.6.1 preshed-3.0.12 pydantic-2.12.5 pydantic-core-2.41.5 pyphen-0.17.2 smart-open-7.5.0 spacy-3.8.11 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.2 textacy-0.13.0 thinc-8.3.10 toolz-1.1.0 tqdm-4.67.1 typer-slim-0.20.0 typing-inspection-0.4.2 wasabi-1.1.3 weasel-0.4.3 wrapt-2.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55659456-8a72-4ee0-a9bc-c5fc9532b885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'√§' == 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f2ac393-f8a5-421c-9b98-42a7322a4f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy.preprocessing as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43883056-cee0-4ccb-a7e2-ee59b2912593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda create -n textacy_env python=3.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a490ce89-1444-4ebf-95b5-ab853f27de9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m√§\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "tp.normalize('√§')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df89593-da71-445f-a5ec-b82be300dcb4",
   "metadata": {},
   "source": [
    "### 8. Handling URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b07947a9-05a0-4d4f-ab5e-f367ce517880",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\SOURAV\\Downloads\\4743416-text_filess\\URL Data.txt\", mode='r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "297ce796-8452-4fe8-b640-80ff26a9aaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urls = \"\"\"\n",
      "Check out my blog at https://www.myblog.com for more posts. You can also visit my profile on https://www.linkedin.com/in/johndoe for professional updates. \n",
      "I recently discovered a great article on https://medium.com/@johnny/technology-and-innovation-abc1234. You can find more about my projects on https://github.com/johndoe. \n",
      "Also, follow us on https://www.twitter.com/techhub for the latest news in tech. Don't forget to subscribe to my YouTube channel at https://www.youtube.com/channel/UC12345678. \n",
      "For any inquiries, please contact me at https://www.contactme.com.\n",
      "\"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c4c26e2-b6c5-4f75-9123-6b5e7154c1e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 16:37:03) [MSC v.1929 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d586732d-85a2-472e-ac09-7739642c9469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a66ce2e-4aeb-4c56-9327-299a4688002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'(www.|http:|https:)\\S+'\n",
    "replace = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a7c4781-4643-4aa0-a8fd-934cb99639e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urls = \"\"\"\n",
      "Check out my blog at   for more posts. You can also visit my profile on   for professional updates. \n",
      "I recently discovered a great article on   You can find more about my projects on   \n",
      "Also, follow us on   for the latest news in tech. Don't forget to subscribe to my YouTube channel at   \n",
      "For any inquiries, please contact me at  \n",
      "\"\"\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(re.sub(pattern, replace, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917e1d99-6b86-4830-92eb-a78e8625142a",
   "metadata": {},
   "source": [
    "### 9. HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4771f9ac-d48b-4653-9e84-3b5c6a1a7c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\SOURAV\\Downloads\\4743416-text_filess\\HTML tags.txt\", 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4eb50880-3df4-43d9-93d7-ee41866343cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ae5cac1-0495-4c5b-a0ce-b165256b65b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'<.*?>'\n",
    "replace = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84664c96-163a-438e-a68d-0af22481ead7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text = ''' \n",
      " \n",
      " \n",
      "     Sample Text Data \n",
      " \n",
      " \n",
      "     Welcome to the Sample Text Data \n",
      "     This is a  paragraph  with some  HTML  tags for  linking . \n",
      "     \n",
      "         Another paragraph in a  &lt;div&gt;  element. \n",
      "         \n",
      "             List item one \n",
      "             List item two \n",
      "             List item three \n",
      "         \n",
      "     \n",
      "     \n",
      "         Contact us at  example@example.com  \n",
      "     \n",
      " \n",
      " \n",
      "'''\n"
     ]
    }
   ],
   "source": [
    "print(re.sub(pattern, replace, text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfacec2-4fa5-499d-b100-1f478fc24ce9",
   "metadata": {},
   "source": [
    "### 10. Handling Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1df0bba9-c413-4b94-aaeb-0988221eb8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'My phone number is 9494944911 and salary is 949999 and address is 979 Flat Room no 4044'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1ceb5cb-2096-4607-9541-3286b36ca784",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = r'[0-9]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e051b59d-8512-4c99-acac-c28ac2a29920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My phone number isand salary is  and address is     Flat Room no     '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(patterns, replace, text).replace('      ', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d9e122-31d4-4d02-a197-5cf17e43c2be",
   "metadata": {},
   "source": [
    "# Advanced Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a721ca98-8813-45e2-8a08-33efeaf99fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: click in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda\\envs\\computervision\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "Downloading regex-2025.11.3-cp312-cp312-win_amd64.whl (277 kB)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.9.2 regex-2025.11.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585992f8-291e-420a-8438-3aecb74e6257",
   "metadata": {},
   "source": [
    "### 11. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b97591fd-4774-4219-aca4-a34a81d08cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "145c611f-5751-41fd-b8e5-cd631e218f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'i am data scientist ,and data analyst'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7beeabd-2e1d-4cbc-9e3e-ed622ff31f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'data', 'scientist', ',', 'and', 'data', 'analyst']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c86821f2-5a98-4e06-ab0f-01d479415a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91d9a3de-3d3e-4788-92b5-4b58e1058223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\SOURAV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f280c110-9b1d-406f-bce4-34762f6ed375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i am data scientist ,and data analyst']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30e9338b-b1a8-4631-bae8-0d3e387f543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aca9bedf-e23b-4cca-9428-d82d42cab3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in c:\\anaconda\\envs\\computervision\\lib\\site-packages (0.10.21)\n",
      "Requirement already satisfied: absl-py in c:\\anaconda\\envs\\computervision\\lib\\site-packages (from mediapipe) (2.3.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from mediapipe) (24.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\anaconda\\envs\\computervision\\lib\\site-packages (from mediapipe) (25.9.23)\n",
      "Requirement already satisfied: jax in c:\\anaconda\\envs\\computervision\\lib\\site-packages (from mediapipe) (0.7.1)\n",
      "Requirement already satisfied: jaxlib in c:\\anaconda\\envs\\computervision\\lib\\site-packages (from mediapipe) (0.7.1)\n",
      "Requirement already satisfied: matplotlib in c:\\anaconda\\envs\\computervision\\lib\\site-packages (from mediapipe) (3.10.7)\n",
      "Requirement already satisfied: numpy<2 in c:\\anaconda\\envs\\computervision\\lib\\site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\anaconda\\envs\\computervision\\lib\\site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\anaconda\\envs\\computervision\\lib\\site-packages (from mediapipe) (4.25.8)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\anaconda\\envs\\computervision\\lib\\site-packages (from mediapipe) (0.5.3)\n",
      "Requirement already satisfied: sentencepiece in c:\\anaconda\\envs\\computervision\\lib\\site-packages (from mediapipe) (0.2.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in c:\\anaconda\\envs\\computervision\\lib\\site-packages (from jax->mediapipe) (0.5.4)\n",
      "Requirement already satisfied: opt_einsum in c:\\anaconda\\envs\\computervision\\lib\\site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.12 in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from jax->mediapipe) (1.14.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\anaconda\\envs\\computervision\\lib\\site-packages (from matplotlib->mediapipe) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\anaconda\\envs\\computervision\\lib\\site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\anaconda\\envs\\computervision\\lib\\site-packages (from matplotlib->mediapipe) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\anaconda\\envs\\computervision\\lib\\site-packages (from matplotlib->mediapipe) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->mediapipe) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\anaconda\\envs\\computervision\\lib\\site-packages (from matplotlib->mediapipe) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\anaconda\\envs\\computervision\\lib\\site-packages (from matplotlib->mediapipe) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sourav\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d73cba5f-0756-4a7d-acfa-d366b7158ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"My name is Sourav and i am learning data Science sourav but Data wants Me\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d3e9e783-db00-4552-9517-098e2858a816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(text.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9413cb4b-fcb2-44f2-a29c-00b8b94dc7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e047fb-2ca5-4faa-823a-d955f6d66f35",
   "metadata": {},
   "source": [
    "### 12. StopWords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c239614f-7483-429b-935b-79228892359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am data scientist and data analyst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e2f5ca02-f751-48e4-a5fe-877ad80a1c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'data', 'scientist', 'and', 'data', 'analyst']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "tokenized_words = word_tokenize(text)\n",
    "tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "38124685-fed8-4bba-a602-da74a858757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4a6f0b99-2480-4bb3-9821-95962f0ce400",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\SOURAV/nltk_data'\n    - 'C:\\\\Anaconda\\\\envs\\\\ComputerVision\\\\nltk_data'\n    - 'C:\\\\Anaconda\\\\envs\\\\ComputerVision\\\\share\\\\nltk_data'\n    - 'C:\\\\Anaconda\\\\envs\\\\ComputerVision\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\SOURAV\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ComputerVision\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ComputerVision\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\SOURAV/nltk_data'\n    - 'C:\\\\Anaconda\\\\envs\\\\ComputerVision\\\\nltk_data'\n    - 'C:\\\\Anaconda\\\\envs\\\\ComputerVision\\\\share\\\\nltk_data'\n    - 'C:\\\\Anaconda\\\\envs\\\\ComputerVision\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\SOURAV\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mstopwords\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwords\u001b[49m()\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ComputerVision\\Lib\\site-packages\\nltk\\corpus\\util.py:120\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ComputerVision\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ComputerVision\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ComputerVision\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\SOURAV/nltk_data'\n    - 'C:\\\\Anaconda\\\\envs\\\\ComputerVision\\\\nltk_data'\n    - 'C:\\\\Anaconda\\\\envs\\\\ComputerVision\\\\share\\\\nltk_data'\n    - 'C:\\\\Anaconda\\\\envs\\\\ComputerVision\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\SOURAV\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "stopwords.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c091745b-4abe-42d2-84a1-6d18799e42c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bb387633-4e27-4da2-b851-7cf9fdf8dbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SOURAV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d011382a-7132-4063-8c39-ef36fd01888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b0bf6c35-7980-4918-a5f3-df12ec05c108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7dc09947-d223-4b03-b6a5-01584e0d0901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'data', 'scientist', 'data', 'analyst']\n"
     ]
    }
   ],
   "source": [
    "without_stopwords = []\n",
    "for i in tokenized_words:\n",
    "    if i not in stopwords.words('english'):\n",
    "        without_stopwords.append(i)\n",
    "\n",
    "print(without_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84a90d8-e763-4c17-bf08-055d8388b2eb",
   "metadata": {},
   "source": [
    "### 13. Stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be21d804-b571-4c67-b51a-85a3b4a4b4c0",
   "metadata": {},
   "source": [
    "##### a. Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "40521170-6ac8-48a5-bcae-762ad5437258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d85b268a-237d-43ba-909e-e6dbc6e64058",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'beautifully'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "92886238-d525-4759-b266-67d50f9746fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beauti'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "ps.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2658ae8b-d4dc-4435-a06c-abbff406baa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beauty'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = LancasterStemmer()\n",
    "ls.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "54b0765a-ff0c-4368-a773-709e71321029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beauti'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = SnowballStemmer(language='english')\n",
    "ss.stem(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e83ae39-d629-4053-83e3-76068bd5b3af",
   "metadata": {},
   "source": [
    "### 14. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3e5616b7-4b2c-4af2-907d-f6a712f3c480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bd2d67f9-f458-486d-b6fd-3db77ef740b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e6a1b212-f051-4271-9b21-66b0749e5095",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\SOURAV/nltk_data'\n    - 'C:\\\\Anaconda\\\\envs\\\\ComputerVision\\\\nltk_data'\n    - 'C:\\\\Anaconda\\\\envs\\\\ComputerVision\\\\share\\\\nltk_data'\n    - 'C:\\\\Anaconda\\\\envs\\\\ComputerVision\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\SOURAV\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ComputerVision\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ComputerVision\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\SOURAV/nltk_data'\n    - 'C:\\\\Anaconda\\\\envs\\\\ComputerVision\\\\nltk_data'\n    - 'C:\\\\Anaconda\\\\envs\\\\ComputerVision\\\\share\\\\nltk_data'\n    - 'C:\\\\Anaconda\\\\envs\\\\ComputerVision\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\SOURAV\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomes\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mwl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ComputerVision\\Lib\\site-packages\\nltk\\stem\\wordnet.py:85\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     61\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Lemmatize `word` by picking the shortest of the possible lemmas,\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03m    using the wordnet corpus reader's built-in _morphy function.\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    :return: The shortest lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ComputerVision\\Lib\\site-packages\\nltk\\stem\\wordnet.py:41\u001b[0m, in \u001b[0;36mWordNetLemmatizer._morphy\u001b[1;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m_morphy() is WordNet's _morphy lemmatizer.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03mIt returns a list of all lemmas found in WordNet.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m['us', 'u']\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordnet \u001b[38;5;28;01mas\u001b[39;00m wn\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(form, pos, check_exceptions)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ComputerVision\\Lib\\site-packages\\nltk\\corpus\\util.py:120\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ComputerVision\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ComputerVision\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Anaconda\\envs\\ComputerVision\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\SOURAV/nltk_data'\n    - 'C:\\\\Anaconda\\\\envs\\\\ComputerVision\\\\nltk_data'\n    - 'C:\\\\Anaconda\\\\envs\\\\ComputerVision\\\\share\\\\nltk_data'\n    - 'C:\\\\Anaconda\\\\envs\\\\ComputerVision\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\SOURAV\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "text = 'comes'\n",
    "wl.lemmatize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "18162a5d-87c8-4408-8ae0-afd0ca3b86a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a79bb504-0f1d-405b-84cc-ef9c273850cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SOURAV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2f4fe33c-fb10-4a9f-ae1b-70090b980304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "50e15cbb-e0cc-4851-b481-1a1b8e6b7e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stemming'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'stemming'\n",
    "wl.lemmatize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "798c24a1-0bc2-4c6a-bad6-6c9384ad8fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    speller = autocorrect.Speller()\n",
    "    stem = PorterStemmer()\n",
    "    lemma = WordNetLemmatizer()\n",
    "    text = text.lower()   #converting to Lower Case\n",
    "    text = speller.autocorrect_sentence(text)   #Corecting the Spelling Mistakes\n",
    "    text = contractions.fix(text)   # dealing with contraction\n",
    "    text = emoji.demojize(text).replace(':',' ')   #Emoji Prediction\n",
    "    text = re.sub(r'<.*?>',' ',text)  #HTML Tags\n",
    "    text = re.sub(r'(www.|http|https)\\+S',' ',text)  #Replacing the URLS\n",
    "    text = re.sub(r\"[^a-zA-Z0-9']\",' ',text)  #Removing the punctuation marks\n",
    "    text = re.sub(r'[0-9] ','',text)    #replacing the Numbers\n",
    "    \n",
    "    text = ' '.join(map(lambda i: chat_words[i] if i in chat_words.keys() else i, text.split()))  #Chat Word Prediction\n",
    "    \n",
    "    text = word_tokenize(text)   #Word Tokenization\n",
    "    text = [i for i in text if i not in stopwords.words('english') ]  #Stop Words Removal\n",
    "    text = [stem.stem(i) for i in text]   #Stemming\n",
    "    text = [lemma.lemmatize(i)for i in text]  #Lemmatization\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cf6373cd-6a17-42f9-a8a5-8b897b07ab73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Using cached emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Using cached emoji-2.15.0-py3-none-any.whl (608 kB)\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8e235dd1-a549-472e-b7db-2caec78f70ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import autocorrect\n",
    "import contractions\n",
    "import re\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c38521a1-2cf8-446f-b0d6-a876adb942a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSOURAV\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m4743416-text_filess\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msample_text_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\SOURAV\\Downloads\\4743416-text_filess\\sample_text_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1e45a294-b405-4465-a85a-49d5a9e0d72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [oh, god, believ, face, tear, joy, face, tear,...\n",
       "1     [best, pizza, ever, pizza, pizza, bestpizzaplac]\n",
       "2    [weather, today, nice, total, love, sun, palm,...\n",
       "3               [gh, monday, morn, worst, weari, face]\n",
       "4                 [finish, 10k, run, feel, great, fit]\n",
       "5       [go, movi, tonight, recommend, clapper, board]\n",
       "6                                  [happen, cri, face]\n",
       "7        [love, new, phone, mobil, phone, fast, sleek]\n",
       "8    [great, time, beach, today, friend, beach, umb...\n",
       "9        [wait, weekend, parti, popper, parti, popper]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].apply(text_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db7a562-8125-4455-9104-02b3699c7da3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
